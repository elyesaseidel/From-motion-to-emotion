{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "265b8ff7-9e82-4014-b8d9-bec168f07fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe13571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e9a2dea-238e-43dc-96a5-fdeb506d2081",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f16ee-736b-43d4-8887-8c21cd18844d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cropping images (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "365b790e-1747-4a7b-846c-a915e23b4f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "path_main ='C:\\\\Users\\\\Olia\\\\Desktop\\\\Lips Reading\\\\Day3\\\\Data(audios)\\\\Test_Data_Spectogram_trimmed' #CHANGE\n",
    "folders = os.listdir(path_main)\n",
    "folders\n",
    "\n",
    "for folder in folders:\n",
    "    path = path_main + fr'\\{folder}'\n",
    "    os.chdir(path)\n",
    "    images = glob.glob(\"*.*\") # CHECK the FORMAT\n",
    "    #print(path.split(r'\\')[])\n",
    "    for image in images:\n",
    "        \n",
    "        # Open the image file\n",
    "        im = Image.open(path + f'\\{image}')\n",
    "        width, height = im.size\n",
    "\n",
    "        # Define the trim box\n",
    "        left = width * 0.15\n",
    "        right = width * 0.9\n",
    "        top = height * 0.15\n",
    "        bottom = height * 0.8\n",
    "\n",
    "        # Crop the image\n",
    "        im = im.crop((left, top, right, bottom))\n",
    "        im.save(path + f'\\{image}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120bf9b0-453f-4570-a447-233641b09c62",
   "metadata": {},
   "source": [
    "### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec694b0e-76c0-4ded-8798-46b5443a76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folders with spectrograms of train data\n",
    "train_data_path = (r'C:\\Users\\Olia\\Desktop\\Lips Reading\\Day3\\Train_data_unified')#(r'C:\\Users\\Olia\\Desktop\\Lips Reading\\Day3\\Data(audios)\\Train_Data_Spectogram_trimmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4209ac82-d458-4a73-a35e-4e981cdab9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1400 files belonging to 7 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading and pre-processing the tain data \n",
    "train_data = tf.keras.utils.image_dataset_from_directory(train_data_path,\n",
    "                                                         #batch_size=100, \n",
    "                                                         label_mode='categorical', \n",
    "                                                         image_size=(256, 256), \n",
    "                                                         labels='inferred', \n",
    "                                                         class_names=['angry', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "265116fa-03fe-4093-959f-6472bb313201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a8f93e4-609a-4cae-a094-0263e02e63fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extracting \n",
    "train_images = np.concatenate([x for x, y in train_data], axis=0)\n",
    "\n",
    "train_labels = np.concatenate([y for x, y in train_data], axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91927e47-a3be-4cb7-b040-79ea13bbbd25",
   "metadata": {},
   "source": [
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd66a99b-9499-47c5-b4e6-243cd516509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = (r'C:\\Users\\Olia\\Desktop\\Lips Reading\\Day3\\Data(audios)\\Test_Data_Spectogram_trimmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eea2df9-f244-44a0-a9cd-ea40e4130c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84 files belonging to 7 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading and pre-processing the test data \n",
    "test_data = tf.keras.utils.image_dataset_from_directory(test_data_path, batch_size=100, label_mode='categorical', image_size=(256, 256), labels='inferred', class_names=['angry', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'])\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0083157-1352-4fc5-8327-290478f6fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting test_images and test_labels\n",
    "test_images = np.concatenate([x for x, y in test_data], axis=0) \n",
    "\n",
    "test_labels = np.concatenate([y for x, y in test_data], axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085b961a-e7e1-496c-b252-a7307add4ba6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data **augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d75b77d2-c66f-4381-8a8e-758f28621d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2564 images belonging to 7 classes.\n",
      "Found 84 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(     # here we use the ImageDataGenerator\n",
    "      #rescale=1./255,\n",
    "      #rotation_range=40,\n",
    "      #width_shift_range=0.2,                # Applaying these all Data Augmentations\n",
    "      #height_shift_range=0.2,\n",
    "      #shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "##train_datagen.fit(train_images)\n",
    "\n",
    "test_datagen = ImageDataGenerator()#(rescale=1./255)\n",
    "\n",
    "##train_datagen.fit(test_images)\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_path,  # this is the target directory\n",
    "        target_size=(256, 256),  # all images will be resized \n",
    "        batch_size=20,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=True,\n",
    "        seed=42)#,\n",
    "        #save_to_dir=r'C:\\Users\\Olia\\Desktop\\Lips Reading\\Day5(Video+Audio_camera, CPU)\\CNN_Checkpoints',\n",
    "        #save_prefix='aug') \n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_path,\n",
    "        target_size=(256, 256),\n",
    "        batch_size=20,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=True,\n",
    "        seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2ea4956-0845-4030-8341-2dca482238dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.ImageDataGenerator at 0x14edddb4d00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f487ed-fb7f-401f-b289-53fdfee86399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_data_gen = image_gen_train.flow_from_directory(batch_size=50,     # Batch siz emeans at a time it takes 100\n",
    "                                                     directory=train_data_path,    # Here we put shuffle= True so tat model doesnt memorise order\n",
    "                                                     shuffle=True,\n",
    "                                                     target_size=(256, 256),\n",
    "                                                     class_mode=\"categorical\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4f359-8b4d-4baf-bcfd-17ea5e16e807",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc11d28e-4266-410e-afc9-53949e8861a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a checkpoint for saving the best performing epoch\n",
    "checkpoint_filepath = r'C:\\Users\\Olia\\Desktop\\Lips Reading\\Day5(Video+Audio_camera, CPU)\\CNN_Checkpoints'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976b03c-eb83-47da-ada5-3da80809a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C:\\Users\\Olia\\Desktop\\Lips Reading\\Day3\\Toronto_Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2170370a-c596-48b6-8313-fd6b86ea819d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "40/40 [==============================] - ETA: 0s - loss: 2.0127 - accuracy: 0.1454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Olia\\Desktop\\Lips Reading\\Day5(Video+Audio_camera, CPU)\\CNN_Checkpoints\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Olia\\Desktop\\Lips Reading\\Day5(Video+Audio_camera, CPU)\\CNN_Checkpoints\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 267s 7s/step - loss: 2.0127 - accuracy: 0.1454 - val_loss: 2.0332 - val_accuracy: 0.1429\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 269s 7s/step - loss: 2.0022 - accuracy: 0.1562 - val_loss: 2.0331 - val_accuracy: 0.1429\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 262s 7s/step - loss: 2.0231 - accuracy: 0.1441 - val_loss: 2.0330 - val_accuracy: 0.1429\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 268s 7s/step - loss: 2.0217 - accuracy: 0.1462 - val_loss: 2.0329 - val_accuracy: 0.1429\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 261s 7s/step - loss: 2.0179 - accuracy: 0.1403 - val_loss: 2.0329 - val_accuracy: 0.1429\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 266s 7s/step - loss: 2.0177 - accuracy: 0.1462 - val_loss: 2.0328 - val_accuracy: 0.1429\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 261s 7s/step - loss: 2.0282 - accuracy: 0.1454 - val_loss: 2.0327 - val_accuracy: 0.1429\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0341 - accuracy: 0.1338 - val_loss: 2.0326 - val_accuracy: 0.1429\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 268s 7s/step - loss: 2.0254 - accuracy: 0.1262 - val_loss: 2.0325 - val_accuracy: 0.1429\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 270s 7s/step - loss: 2.0031 - accuracy: 0.1500 - val_loss: 2.0324 - val_accuracy: 0.1429\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 262s 7s/step - loss: 2.0320 - accuracy: 0.1288 - val_loss: 2.0323 - val_accuracy: 0.1429\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 261s 7s/step - loss: 2.0242 - accuracy: 0.1467 - val_loss: 2.0323 - val_accuracy: 0.1429\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0197 - accuracy: 0.1550 - val_loss: 2.0322 - val_accuracy: 0.1429\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 268s 7s/step - loss: 2.0201 - accuracy: 0.1437 - val_loss: 2.0321 - val_accuracy: 0.1429\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 275s 7s/step - loss: 2.0206 - accuracy: 0.1500 - val_loss: 2.0320 - val_accuracy: 0.1429\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 271s 7s/step - loss: 2.0195 - accuracy: 0.1513 - val_loss: 2.0319 - val_accuracy: 0.1429\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 264s 7s/step - loss: 2.0378 - accuracy: 0.1450 - val_loss: 2.0318 - val_accuracy: 0.1429\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 261s 7s/step - loss: 2.0104 - accuracy: 0.1569 - val_loss: 2.0318 - val_accuracy: 0.1429\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 266s 7s/step - loss: 2.0058 - accuracy: 0.1575 - val_loss: 2.0317 - val_accuracy: 0.1429\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 266s 7s/step - loss: 2.0287 - accuracy: 0.1425 - val_loss: 2.0316 - val_accuracy: 0.1429\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0497 - accuracy: 0.1325 - val_loss: 2.0315 - val_accuracy: 0.1429\n",
      "Epoch 22/50\n",
      "40/40 [==============================] - 262s 7s/step - loss: 2.0045 - accuracy: 0.1645 - val_loss: 2.0314 - val_accuracy: 0.1429\n",
      "Epoch 23/50\n",
      "40/40 [==============================] - 266s 7s/step - loss: 2.0238 - accuracy: 0.1450 - val_loss: 2.0313 - val_accuracy: 0.1429\n",
      "Epoch 24/50\n",
      "40/40 [==============================] - 264s 7s/step - loss: 2.0002 - accuracy: 0.1600 - val_loss: 2.0313 - val_accuracy: 0.1429\n",
      "Epoch 25/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0237 - accuracy: 0.1300 - val_loss: 2.0312 - val_accuracy: 0.1429\n",
      "Epoch 26/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0139 - accuracy: 0.1550 - val_loss: 2.0311 - val_accuracy: 0.1429\n",
      "Epoch 27/50\n",
      "40/40 [==============================] - 262s 7s/step - loss: 2.0084 - accuracy: 0.1556 - val_loss: 2.0310 - val_accuracy: 0.1429\n",
      "Epoch 28/50\n",
      "40/40 [==============================] - 267s 7s/step - loss: 2.0299 - accuracy: 0.1500 - val_loss: 2.0309 - val_accuracy: 0.1429\n",
      "Epoch 29/50\n",
      "40/40 [==============================] - 264s 7s/step - loss: 2.0469 - accuracy: 0.1338 - val_loss: 2.0309 - val_accuracy: 0.1429\n",
      "Epoch 30/50\n",
      "40/40 [==============================] - 260s 6s/step - loss: 2.0198 - accuracy: 0.1531 - val_loss: 2.0308 - val_accuracy: 0.1429\n",
      "Epoch 31/50\n",
      "40/40 [==============================] - 264s 7s/step - loss: 2.0170 - accuracy: 0.1612 - val_loss: 2.0307 - val_accuracy: 0.1429\n",
      "Epoch 32/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0248 - accuracy: 0.1475 - val_loss: 2.0306 - val_accuracy: 0.1429\n",
      "Epoch 33/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0187 - accuracy: 0.1500 - val_loss: 2.0305 - val_accuracy: 0.1429\n",
      "Epoch 34/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 1.9999 - accuracy: 0.1562 - val_loss: 2.0305 - val_accuracy: 0.1429\n",
      "Epoch 35/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0202 - accuracy: 0.1513 - val_loss: 2.0304 - val_accuracy: 0.1429\n",
      "Epoch 36/50\n",
      "40/40 [==============================] - 260s 6s/step - loss: 2.0243 - accuracy: 0.1531 - val_loss: 2.0303 - val_accuracy: 0.1429\n",
      "Epoch 37/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0018 - accuracy: 0.1437 - val_loss: 2.0302 - val_accuracy: 0.1429\n",
      "Epoch 38/50\n",
      "40/40 [==============================] - 260s 6s/step - loss: 2.0172 - accuracy: 0.1582 - val_loss: 2.0301 - val_accuracy: 0.1429\n",
      "Epoch 39/50\n",
      "40/40 [==============================] - 264s 7s/step - loss: 2.0300 - accuracy: 0.1412 - val_loss: 2.0300 - val_accuracy: 0.1429\n",
      "Epoch 40/50\n",
      "40/40 [==============================] - 263s 7s/step - loss: 2.0153 - accuracy: 0.1500 - val_loss: 2.0300 - val_accuracy: 0.1429\n",
      "Epoch 41/50\n",
      "40/40 [==============================] - 264s 7s/step - loss: 1.9934 - accuracy: 0.1587 - val_loss: 2.0299 - val_accuracy: 0.1429\n",
      "Epoch 42/50\n",
      "40/40 [==============================] - 264s 7s/step - loss: 2.0217 - accuracy: 0.1538 - val_loss: 2.0298 - val_accuracy: 0.1429\n",
      "Epoch 43/50\n",
      "40/40 [==============================] - 266s 7s/step - loss: 2.0041 - accuracy: 0.1513 - val_loss: 2.0297 - val_accuracy: 0.1429\n",
      "Epoch 44/50\n",
      "40/40 [==============================] - 267s 7s/step - loss: 2.0219 - accuracy: 0.1562 - val_loss: 2.0297 - val_accuracy: 0.1429\n",
      "Epoch 45/50\n",
      "40/40 [==============================] - 259s 6s/step - loss: 2.0203 - accuracy: 0.1429 - val_loss: 2.0296 - val_accuracy: 0.1429\n",
      "Epoch 46/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0059 - accuracy: 0.1550 - val_loss: 2.0295 - val_accuracy: 0.1429\n",
      "Epoch 47/50\n",
      "40/40 [==============================] - 264s 7s/step - loss: 2.0022 - accuracy: 0.1675 - val_loss: 2.0294 - val_accuracy: 0.1429\n",
      "Epoch 48/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0185 - accuracy: 0.1450 - val_loss: 2.0294 - val_accuracy: 0.1429\n",
      "Epoch 49/50\n",
      "40/40 [==============================] - 265s 7s/step - loss: 2.0126 - accuracy: 0.1550 - val_loss: 2.0293 - val_accuracy: 0.1429\n",
      "Epoch 50/50\n",
      "40/40 [==============================] - 261s 7s/step - loss: 2.0153 - accuracy: 0.1492 - val_loss: 2.0292 - val_accuracy: 0.1429\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    #tf.keras.layers.Input(shape=(256, 256)),\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (4, 4), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, (4, 4), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    #tf.keras.layers.Dropout(rate=0.3, seed=15),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(rate=0.2, seed=15),\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(rate=0.2, seed=15),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(30, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "#variant unu\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=1e-5)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n",
    "\n",
    "#variant doi\n",
    "##model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator,\n",
    "                    #train_images, train_labels, \n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=40,\n",
    "                    #batch_size=50, \n",
    "                    shuffle=True, \n",
    "                    callbacks=[model_checkpoint_callback],\n",
    "                    validation_data=test_generator)#(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d346842-2994-431a-a5bd-b985762ce8ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "568f7021-a9a3-43a2-91ff-6a4ed6682e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1dd5d3e77f0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the model's performance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9a8d5b1-91f7-4954-ae7b-53082bd54e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 5s - loss: 2.0290 - accuracy: 0.1429 - 5s/epoch - 2s/step\n",
      "0.1428571492433548\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d0dc1-abf0-4353-adad-6e4ee8c5188d",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c308e08-7792-44a2-b260-785c4fe2817b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 258ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.14437829, 0.26099533, 0.17664614, 0.07505419, 0.09884663,\n",
       "        0.08808781, 0.15599158]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the emotion\n",
    "image = tf.keras.utils.load_img(r'C:\\Users\\Olia\\Desktop\\03-01-05-01-01-01-12.jpeg', target_size=(256, 256)) #YOUR IMAGE PSATH\n",
    "input_arr = tf.keras.utils.img_to_array(image)\n",
    "input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "predictions = model.predict(input_arr)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4748c9e1-3121-460b-963a-fbc9735e31b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_index = np.argmax(predictions)\n",
    "max_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f8720-e220-466e-84f4-bb960bb43dfa",
   "metadata": {},
   "source": [
    "## Decoding the ptredicted label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67935f2-edbc-4d18-8a7f-4125c4919e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disgust']\n"
     ]
    }
   ],
   "source": [
    "# Define a sample one-hot encoding\n",
    "##onehot_encoded = np.array([\"PREDICTION\"]) #[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "# Get the indices where the values are 1\n",
    "##index_of_max = np.argmax(onehot_encoded, axis=1)\n",
    "\n",
    "# Create a list of labels to match the indices\n",
    "labels = ['angry', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "\n",
    "# Use the indices to get the labels\n",
    "decoded_labels = [labels[max_index]]# for index in index_of_max]\n",
    "\n",
    "# The decoded labels\n",
    "print(decoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5948d19-f8a5-4e36-93aa-a905ea39ea70",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519f60d-e663-4019-97ba-3d13f1399acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Dummy_CNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f95985e-e40c-46a3-973b-420cf226e34a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\conv2d\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_3\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\conv2d_4\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_2\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_3\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_4\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\flatten\n",
      "......vars\n",
      "...layers\\max_pooling2d\n",
      "......vars\n",
      "...layers\\max_pooling2d_1\n",
      "......vars\n",
      "...layers\\max_pooling2d_2\n",
      "......vars\n",
      "...layers\\max_pooling2d_3\n",
      "......vars\n",
      "...layers\\max_pooling2d_4\n",
      "......vars\n",
      "...layers\\rescaling\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...metrics\\mean_metric_wrapper\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-02-06 12:42:08         7063\n",
      "metadata.json                                  2023-02-06 12:42:08           64\n",
      "variables.h5                                   2023-02-06 12:42:08      5707096\n"
     ]
    }
   ],
   "source": [
    "# save the model with pickle\n",
    "import pickle\n",
    "filename = 'CNN_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.predict([text])[0]\n",
    "# print(result)\n",
    "#pickle.dump(pipe_lr, open('/content/gdrive/MyDrive/From Motion to Emotion/emo_text_model.sav' , 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
