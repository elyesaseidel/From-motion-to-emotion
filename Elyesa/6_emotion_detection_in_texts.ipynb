{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97a8ac2-fdff-46bc-bc11-2a1f7766c8e5",
   "metadata": {},
   "source": [
    "# Emotion detection in text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd9dd2-ceaf-4364-a163-2bf5abe306ca",
   "metadata": {},
   "source": [
    "## 1. Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef2ffbb-7a47-4c29-af7d-ecdf83003dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f039743-7ac0-4232-951a-6aac33f1fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c00cef4-b297-49af-918c-983f381ae91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral                                             Why ? \n",
       "1      joy    Sage Act upgrade on my to do list for tommorow.\n",
       "2  sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...\n",
       "3      joy   Such an eye ! The true hazel eye-and so brill...\n",
       "4      joy  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f625f5ca-8c92-4235-8a80-1af19b8c1cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         11045\n",
       "sadness      6722\n",
       "fear         5410\n",
       "anger        4297\n",
       "surprise     4062\n",
       "neutral      2254\n",
       "disgust       856\n",
       "shame         146\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe446c4-64b1-4ca4-afc8-05d4f3c8299c",
   "metadata": {},
   "source": [
    "## 2. Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cabaed8a-2f81-41fa-a228-cd9cf7215fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral\\nfear\\nangry\\nsad\\nhappy\\nsurprise\\ndisgust'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Justin\n",
    "'''neutral\n",
    "fear\n",
    "angry\n",
    "sad\n",
    "happy\n",
    "surprise\n",
    "disgust'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc10b25-0a28-4806-a251-51492a911d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neattext\n",
      "  Downloading neattext-0.1.3-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: neattext\n",
      "Successfully installed neattext-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876f1a24-d943-48e8-abf0-157b88ce039a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d157d6-268a-4c8f-af40-cc0596818a91",
   "metadata": {},
   "source": [
    "To use neattext, we list all the methods and attributes used by neattext for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e359c94e-628b-4fc9-a3c7-efa0e01215a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTC_ADDRESS_REGEX',\n",
       " 'CURRENCY_REGEX',\n",
       " 'CURRENCY_SYMB_REGEX',\n",
       " 'Counter',\n",
       " 'DATE_REGEX',\n",
       " 'EMAIL_REGEX',\n",
       " 'EMOJI_REGEX',\n",
       " 'HASTAG_REGEX',\n",
       " 'MASTERCard_REGEX',\n",
       " 'MD5_SHA_REGEX',\n",
       " 'MOST_COMMON_PUNCT_REGEX',\n",
       " 'NUMBERS_REGEX',\n",
       " 'PHONE_REGEX',\n",
       " 'PoBOX_REGEX',\n",
       " 'SPECIAL_CHARACTERS_REGEX',\n",
       " 'STOPWORDS',\n",
       " 'STOPWORDS_de',\n",
       " 'STOPWORDS_en',\n",
       " 'STOPWORDS_es',\n",
       " 'STOPWORDS_fr',\n",
       " 'STOPWORDS_ru',\n",
       " 'STOPWORDS_yo',\n",
       " 'STREET_ADDRESS_REGEX',\n",
       " 'TextFrame',\n",
       " 'URL_PATTERN',\n",
       " 'USER_HANDLES_REGEX',\n",
       " 'VISACard_REGEX',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__generate_text',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numbers_dict',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_lex_richness_herdan',\n",
       " '_lex_richness_maas_ttr',\n",
       " 'clean_text',\n",
       " 'defaultdict',\n",
       " 'digit2words',\n",
       " 'extract_btc_address',\n",
       " 'extract_currencies',\n",
       " 'extract_currency_symbols',\n",
       " 'extract_dates',\n",
       " 'extract_emails',\n",
       " 'extract_emojis',\n",
       " 'extract_hashtags',\n",
       " 'extract_html_tags',\n",
       " 'extract_mastercard_addr',\n",
       " 'extract_md5sha',\n",
       " 'extract_numbers',\n",
       " 'extract_pattern',\n",
       " 'extract_phone_numbers',\n",
       " 'extract_postoffice_box',\n",
       " 'extract_shortwords',\n",
       " 'extract_special_characters',\n",
       " 'extract_stopwords',\n",
       " 'extract_street_address',\n",
       " 'extract_terms_in_bracket',\n",
       " 'extract_urls',\n",
       " 'extract_userhandles',\n",
       " 'extract_visacard_addr',\n",
       " 'fix_contractions',\n",
       " 'generate_sentence',\n",
       " 'hamming_distance',\n",
       " 'inverse_df',\n",
       " 'lexical_richness',\n",
       " 'markov_chain',\n",
       " 'math',\n",
       " 'nlargest',\n",
       " 'normalize',\n",
       " 'num2words',\n",
       " 'random',\n",
       " 're',\n",
       " 'read_txt',\n",
       " 'remove_accents',\n",
       " 'remove_bad_quotes',\n",
       " 'remove_btc_address',\n",
       " 'remove_currencies',\n",
       " 'remove_currency_symbols',\n",
       " 'remove_custom_pattern',\n",
       " 'remove_custom_words',\n",
       " 'remove_dates',\n",
       " 'remove_emails',\n",
       " 'remove_emojis',\n",
       " 'remove_hashtags',\n",
       " 'remove_html_tags',\n",
       " 'remove_mastercard_addr',\n",
       " 'remove_md5sha',\n",
       " 'remove_multiple_spaces',\n",
       " 'remove_non_ascii',\n",
       " 'remove_numbers',\n",
       " 'remove_phone_numbers',\n",
       " 'remove_postoffice_box',\n",
       " 'remove_puncts',\n",
       " 'remove_punctuations',\n",
       " 'remove_shortwords',\n",
       " 'remove_special_characters',\n",
       " 'remove_stopwords',\n",
       " 'remove_street_address',\n",
       " 'remove_terms_in_bracket',\n",
       " 'remove_urls',\n",
       " 'remove_userhandles',\n",
       " 'remove_visacard_addr',\n",
       " 'replace_bad_quotes',\n",
       " 'replace_currencies',\n",
       " 'replace_currency_symbols',\n",
       " 'replace_dates',\n",
       " 'replace_emails',\n",
       " 'replace_emojis',\n",
       " 'replace_numbers',\n",
       " 'replace_phone_numbers',\n",
       " 'replace_special_characters',\n",
       " 'replace_term',\n",
       " 'replace_urls',\n",
       " 'string',\n",
       " 'term_freq',\n",
       " 'to_txt',\n",
       " 'unicodedata',\n",
       " 'word_freq',\n",
       " 'word_length_freq']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9faa603-05e6-4065-9188-45c855ee2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user handles\n",
    "df['Clean_Text'] = df['Text'].apply(nfx.remove_userhandles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218b89a-6c77-4fa3-92f3-3b8ab375b280",
   "metadata": {},
   "source": [
    "We also use apply() to add remove_stopwords. We save the cleaned dataset into a new column named Clean_Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "978c9405-679a-47e2-8032-f2dceed7cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "df['Clean_Text'] = df['Clean_Text'].apply(nfx.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efac95e6-fc55-429c-a354-700aee7fb279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>Sage Act upgrade list tommorow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td>WAY HOMEGIRL BABY FUNERAL!!! MAN HATE FUNERALS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>eye ! true hazel eye-and brilliant ! Regular f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>ugh babe.. hugggzzz u .! babe naamazed nga ako...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34787</th>\n",
       "      <td>surprise</td>\n",
       "      <td>@MichelGW have you gift! Hope you like it! It'...</td>\n",
       "      <td>gift! Hope like it! hand wear ! It'll warm! Lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34788</th>\n",
       "      <td>joy</td>\n",
       "      <td>The world didnt give it to me..so the world MO...</td>\n",
       "      <td>world didnt me..so world DEFINITELY cnt away!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34789</th>\n",
       "      <td>anger</td>\n",
       "      <td>A man robbed me today .</td>\n",
       "      <td>man robbed today .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34790</th>\n",
       "      <td>fear</td>\n",
       "      <td>Youu call it JEALOUSY, I call it of #Losing YO...</td>\n",
       "      <td>Youu JEALOUSY, #Losing YOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34791</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I think about you baby, and I dream about you ...</td>\n",
       "      <td>think baby, dream time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34792 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                               Text  \\\n",
       "0       neutral                                             Why ?    \n",
       "1           joy    Sage Act upgrade on my to do list for tommorow.   \n",
       "2       sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3           joy   Such an eye ! The true hazel eye-and so brill...   \n",
       "4           joy  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "...         ...                                                ...   \n",
       "34787  surprise  @MichelGW have you gift! Hope you like it! It'...   \n",
       "34788       joy  The world didnt give it to me..so the world MO...   \n",
       "34789     anger                           A man robbed me today .    \n",
       "34790      fear  Youu call it JEALOUSY, I call it of #Losing YO...   \n",
       "34791   sadness  I think about you baby, and I dream about you ...   \n",
       "\n",
       "                                              Clean_Text  \n",
       "0                                                      ?  \n",
       "1                        Sage Act upgrade list tommorow.  \n",
       "2      WAY HOMEGIRL BABY FUNERAL!!! MAN HATE FUNERALS...  \n",
       "3      eye ! true hazel eye-and brilliant ! Regular f...  \n",
       "4      ugh babe.. hugggzzz u .! babe naamazed nga ako...  \n",
       "...                                                  ...  \n",
       "34787    gift! Hope like it! hand wear ! It'll warm! Lol  \n",
       "34788    world didnt me..so world DEFINITELY cnt away!!!  \n",
       "34789                                 man robbed today .  \n",
       "34790                      Youu JEALOUSY, #Losing YOU...  \n",
       "34791                             think baby, dream time  \n",
       "\n",
       "[34792 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a94e44-ac5b-427a-85e0-d747811fefd7",
   "metadata": {},
   "source": [
    "## 3. Importing ML packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b90e16d-7c3a-4868-89d9-597ea61e2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8acda4-f1c8-4de4-8f5e-22f7529697b3",
   "metadata": {},
   "source": [
    "## 4. Model features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb560a-ab3c-4d39-b7dc-03d4e8043e26",
   "metadata": {},
   "source": [
    "**Features** are the attributes and variables extracted from the dataset. These extracted features are used as inputs to the model during training enabling model learning. Our features are present in the Clean_Text column.\n",
    "\n",
    "**Labels** are the output or the target variable. Our label is the Emotion column, and this is what the model is predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e1fbbaa-3863-4fdc-a976-a85a60429522",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfeatures = df['Clean_Text']\n",
    "ylabels = df['Emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a55b7e-cba4-4268-a613-8a127f50c40a",
   "metadata": {},
   "source": [
    "## 5. Data splitting and pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35e25f57-a2a0-4e9e-9561-06ed859a83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "x_train,x_test,y_train,y_test = train_test_split(Xfeatures,ylabels,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080ccda-783d-43f1-8a7c-22c6a138803b",
   "metadata": {},
   "source": [
    "The first stage is the CountVectorizer process. This stage converts the raw text dataset into a matrix of numbers that a machine can understand.\n",
    "\n",
    "The second stage is the model training process using the LogisticRegression algorithm. In this stage, the model learns from the dataset. During training, it understands patterns, gains knowledge, and uses the knowledge to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4feeec2e-b9de-4977-92c9-df879c4af40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e3d58d9-eebd-4af8-8fc2-8046a1303961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogistiticRegression pipeline\n",
    "pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb3af0-7588-40fa-bc10-514ce33b1a25",
   "metadata": {},
   "source": [
    "## 7. Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0de0e-7625-462b-bf3f-57411507754f",
   "metadata": {},
   "source": [
    "After initializing the two stages, we need to fit these stages into our dataset. We will use the train set dataset, which is specified as x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26a9928f-60f8-41a0-ba53-9fba606538c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and fit data\n",
    "pipe_lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21870e50-d837-43b4-ae22-bd293f9bca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5a909e5-f16a-40f8-b74c-765dda30b922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6200421536692853"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check accuracy\n",
    "pipe_lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "156f06e0-118c-4781-b868-1f02a33dc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "sample1 = \"This chocolate was very sweet it made me happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "195a5db1-6d9b-40a3-a889-49ffd4417419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual prediction\n",
    "pipe_lr.predict([sample1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d5aa822-5f22-44b5-99e0-495551c1fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"this is all wrong I shouldn't be up here I should be back in school on the other side of the ocean yet you all come to us young people for Hope how dare you you have stolen my dreams and my childhood with your empty words and yet I'm one of the lucky ones people are suffering people are dying entire ecosystems are collapsing we are in the beginning of a mass extinction and all you can talk about is the money and fairy tales of Eternal economic growth how dare you\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee9f9217-bdb3-441e-a617-637e064e3b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fear'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.predict([text_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fcbef3b6-0ee8-4501-a52b-c0cdda4bec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"thank you thank you thank you thank you to the academy for this all 6,000 members thank you to the other nominees all these performances were impeccable in my opinion I didn't see a false note anywhere I want to thank valet or director Jennifer garnered with daily there's a few things about three things to my account that I need each day one of them is something to look up to another is something to look forward to in another is someone to chase now first off I want to thank God because that's who I look up to he's great my life with opportunities that I know are not of my hand or any other human hand he is showing me that it's a scientific fact that gratitude reciprocates\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e4eefef-5444-48ed-bff2-e055e2dd50ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['surprise'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.predict([text_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac07bbb0-9f73-4dc2-b1cf-ecdafd2978c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"I used to think the whole purpose of life was pursuing happiness everyone said the path to happiness was success so I searched for that ideal job that perfect boyfriend that beautiful apartment but instead of ever feeling fulfilled I felt anxious and the drift and I wasn't alone my friends they struggled with this too\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc0c0e69-97f0-4e41-96cc-0f885d51db5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy'], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.predict([text_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c878819-87de-4200-81b7-7faccca4c9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
