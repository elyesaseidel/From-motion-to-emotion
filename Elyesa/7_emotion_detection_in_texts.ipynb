{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97a8ac2-fdff-46bc-bc11-2a1f7766c8e5",
   "metadata": {},
   "source": [
    "# Emotion detection in text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9c6fa-77d2-4258-b476-f60c64d91573",
   "metadata": {},
   "source": [
    "We will use Neattext and Scikit-learn to build our model. Neattext is a Python library used to preprocess our dataset. Neattext will clean the text dataset by removing stop words and other noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b9e90-42c2-42cc-a558-6ecafaccea5e",
   "metadata": {},
   "source": [
    "source: https://www.section.io/engineering-education/nlp-based-detection-model-using-neattext-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd9dd2-ceaf-4364-a163-2bf5abe306ca",
   "metadata": {},
   "source": [
    "## 1. Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef2ffbb-7a47-4c29-af7d-ecdf83003dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f039743-7ac0-4232-951a-6aac33f1fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"7_emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c00cef4-b297-49af-918c-983f381ae91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral                                             Why ? \n",
       "1      joy    Sage Act upgrade on my to do list for tommorow.\n",
       "2  sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...\n",
       "3      joy   Such an eye ! The true hazel eye-and so brill...\n",
       "4      joy  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f625f5ca-8c92-4235-8a80-1af19b8c1cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         11045\n",
       "sadness      6722\n",
       "fear         5410\n",
       "anger        4297\n",
       "surprise     4062\n",
       "neutral      2254\n",
       "disgust       856\n",
       "shame         146\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e16e062c-3296-4c0f-a8a1-52513c8bb93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'shame'\n",
    "df.drop(df[df.Emotion == 'shame'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "432adcdd-628f-4590-aab9-442e63773ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joy         11045\n",
       "sadness      6722\n",
       "fear         5410\n",
       "anger        4297\n",
       "surprise     4062\n",
       "neutral      2254\n",
       "disgust       856\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe446c4-64b1-4ca4-afc8-05d4f3c8299c",
   "metadata": {},
   "source": [
    "## 2. Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cabaed8a-2f81-41fa-a228-cd9cf7215fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral\\nfear\\nangry\\nsad\\nhappy\\nsurprise\\ndisgust'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Justin\n",
    "'''neutral\n",
    "fear\n",
    "angry\n",
    "sad\n",
    "happy\n",
    "surprise\n",
    "disgust'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abc10b25-0a28-4806-a251-51492a911d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neattext in /opt/anaconda3/lib/python3.9/site-packages (0.1.3)\n"
     ]
    }
   ],
   "source": [
    "#!pip install neattext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876f1a24-d943-48e8-abf0-157b88ce039a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import neattext.functions as nfx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d157d6-268a-4c8f-af40-cc0596818a91",
   "metadata": {},
   "source": [
    "To use neattext, we list all the methods and attributes used by neattext for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e359c94e-628b-4fc9-a3c7-efa0e01215a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTC_ADDRESS_REGEX',\n",
       " 'CURRENCY_REGEX',\n",
       " 'CURRENCY_SYMB_REGEX',\n",
       " 'Counter',\n",
       " 'DATE_REGEX',\n",
       " 'EMAIL_REGEX',\n",
       " 'EMOJI_REGEX',\n",
       " 'HASTAG_REGEX',\n",
       " 'MASTERCard_REGEX',\n",
       " 'MD5_SHA_REGEX',\n",
       " 'MOST_COMMON_PUNCT_REGEX',\n",
       " 'NUMBERS_REGEX',\n",
       " 'PHONE_REGEX',\n",
       " 'PoBOX_REGEX',\n",
       " 'SPECIAL_CHARACTERS_REGEX',\n",
       " 'STOPWORDS',\n",
       " 'STOPWORDS_de',\n",
       " 'STOPWORDS_en',\n",
       " 'STOPWORDS_es',\n",
       " 'STOPWORDS_fr',\n",
       " 'STOPWORDS_ru',\n",
       " 'STOPWORDS_yo',\n",
       " 'STREET_ADDRESS_REGEX',\n",
       " 'TextFrame',\n",
       " 'URL_PATTERN',\n",
       " 'USER_HANDLES_REGEX',\n",
       " 'VISACard_REGEX',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__generate_text',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__numbers_dict',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_lex_richness_herdan',\n",
       " '_lex_richness_maas_ttr',\n",
       " 'clean_text',\n",
       " 'defaultdict',\n",
       " 'digit2words',\n",
       " 'extract_btc_address',\n",
       " 'extract_currencies',\n",
       " 'extract_currency_symbols',\n",
       " 'extract_dates',\n",
       " 'extract_emails',\n",
       " 'extract_emojis',\n",
       " 'extract_hashtags',\n",
       " 'extract_html_tags',\n",
       " 'extract_mastercard_addr',\n",
       " 'extract_md5sha',\n",
       " 'extract_numbers',\n",
       " 'extract_pattern',\n",
       " 'extract_phone_numbers',\n",
       " 'extract_postoffice_box',\n",
       " 'extract_shortwords',\n",
       " 'extract_special_characters',\n",
       " 'extract_stopwords',\n",
       " 'extract_street_address',\n",
       " 'extract_terms_in_bracket',\n",
       " 'extract_urls',\n",
       " 'extract_userhandles',\n",
       " 'extract_visacard_addr',\n",
       " 'fix_contractions',\n",
       " 'generate_sentence',\n",
       " 'hamming_distance',\n",
       " 'inverse_df',\n",
       " 'lexical_richness',\n",
       " 'markov_chain',\n",
       " 'math',\n",
       " 'nlargest',\n",
       " 'normalize',\n",
       " 'num2words',\n",
       " 'random',\n",
       " 're',\n",
       " 'read_txt',\n",
       " 'remove_accents',\n",
       " 'remove_bad_quotes',\n",
       " 'remove_btc_address',\n",
       " 'remove_currencies',\n",
       " 'remove_currency_symbols',\n",
       " 'remove_custom_pattern',\n",
       " 'remove_custom_words',\n",
       " 'remove_dates',\n",
       " 'remove_emails',\n",
       " 'remove_emojis',\n",
       " 'remove_hashtags',\n",
       " 'remove_html_tags',\n",
       " 'remove_mastercard_addr',\n",
       " 'remove_md5sha',\n",
       " 'remove_multiple_spaces',\n",
       " 'remove_non_ascii',\n",
       " 'remove_numbers',\n",
       " 'remove_phone_numbers',\n",
       " 'remove_postoffice_box',\n",
       " 'remove_puncts',\n",
       " 'remove_punctuations',\n",
       " 'remove_shortwords',\n",
       " 'remove_special_characters',\n",
       " 'remove_stopwords',\n",
       " 'remove_street_address',\n",
       " 'remove_terms_in_bracket',\n",
       " 'remove_urls',\n",
       " 'remove_userhandles',\n",
       " 'remove_visacard_addr',\n",
       " 'replace_bad_quotes',\n",
       " 'replace_currencies',\n",
       " 'replace_currency_symbols',\n",
       " 'replace_dates',\n",
       " 'replace_emails',\n",
       " 'replace_emojis',\n",
       " 'replace_numbers',\n",
       " 'replace_phone_numbers',\n",
       " 'replace_special_characters',\n",
       " 'replace_term',\n",
       " 'replace_urls',\n",
       " 'string',\n",
       " 'term_freq',\n",
       " 'to_txt',\n",
       " 'unicodedata',\n",
       " 'word_freq',\n",
       " 'word_length_freq']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nfx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532286c-08d0-41e2-8c05-5a568eee6c17",
   "metadata": {},
   "source": [
    "We are interested in only two methods from the list, the remove_stopwords and remove_userhandles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789ef93-760a-4083-8616-3e509c438588",
   "metadata": {},
   "source": [
    "The dataset contains some Twitter handles of different users. These handles are unnecessary and considered as noise to our dataset. We use the remove_userhandles method to remove them from our dataset. We use the apply() method to add remove_userhandles. We save the cleaned dataset into a new column named Clean_Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9faa603-05e6-4065-9188-45c855ee2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user handles\n",
    "df['Clean_Text'] = df['Text'].apply(nfx.remove_userhandles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218b89a-6c77-4fa3-92f3-3b8ab375b280",
   "metadata": {},
   "source": [
    "We also use apply() to add remove_stopwords. We save the cleaned dataset into a new column named Clean_Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "978c9405-679a-47e2-8032-f2dceed7cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "#df['Clean_Text'] = df['Clean_Text'].apply(nfx.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "efac95e6-fc55-429c-a354-700aee7fb279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Why ?</td>\n",
       "      <td>Why ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>joy</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "      <td>Sage Act upgrade on my to do list for tommorow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "      <td>ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "      <td>Such an eye ! The true hazel eye-and so brill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "      <td>@Iluvmiasantos ugh babe.. hugggzzz for u .!  b...</td>\n",
       "      <td>ugh babe.. hugggzzz for u .!  babe naamazed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34787</th>\n",
       "      <td>surprise</td>\n",
       "      <td>@MichelGW have you gift! Hope you like it! It'...</td>\n",
       "      <td>have you gift! Hope you like it! It's hand m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34788</th>\n",
       "      <td>joy</td>\n",
       "      <td>The world didnt give it to me..so the world MO...</td>\n",
       "      <td>The world didnt give it to me..so the world MO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34789</th>\n",
       "      <td>anger</td>\n",
       "      <td>A man robbed me today .</td>\n",
       "      <td>A man robbed me today .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34790</th>\n",
       "      <td>fear</td>\n",
       "      <td>Youu call it JEALOUSY, I call it of #Losing YO...</td>\n",
       "      <td>Youu call it JEALOUSY, I call it of #Losing YO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34791</th>\n",
       "      <td>sadness</td>\n",
       "      <td>I think about you baby, and I dream about you ...</td>\n",
       "      <td>I think about you baby, and I dream about you ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34646 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion                                               Text  \\\n",
       "0       neutral                                             Why ?    \n",
       "1           joy    Sage Act upgrade on my to do list for tommorow.   \n",
       "2       sadness  ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...   \n",
       "3           joy   Such an eye ! The true hazel eye-and so brill...   \n",
       "4           joy  @Iluvmiasantos ugh babe.. hugggzzz for u .!  b...   \n",
       "...         ...                                                ...   \n",
       "34787  surprise  @MichelGW have you gift! Hope you like it! It'...   \n",
       "34788       joy  The world didnt give it to me..so the world MO...   \n",
       "34789     anger                           A man robbed me today .    \n",
       "34790      fear  Youu call it JEALOUSY, I call it of #Losing YO...   \n",
       "34791   sadness  I think about you baby, and I dream about you ...   \n",
       "\n",
       "                                              Clean_Text  \n",
       "0                                                 Why ?   \n",
       "1        Sage Act upgrade on my to do list for tommorow.  \n",
       "2      ON THE WAY TO MY HOMEGIRL BABY FUNERAL!!! MAN ...  \n",
       "3       Such an eye ! The true hazel eye-and so brill...  \n",
       "4        ugh babe.. hugggzzz for u .!  babe naamazed ...  \n",
       "...                                                  ...  \n",
       "34787    have you gift! Hope you like it! It's hand m...  \n",
       "34788  The world didnt give it to me..so the world MO...  \n",
       "34789                           A man robbed me today .   \n",
       "34790  Youu call it JEALOUSY, I call it of #Losing YO...  \n",
       "34791  I think about you baby, and I dream about you ...  \n",
       "\n",
       "[34646 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a94e44-ac5b-427a-85e0-d747811fefd7",
   "metadata": {},
   "source": [
    "## 3. Importing ML packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57caa4e0-19f3-4708-a779-a26020909eda",
   "metadata": {},
   "source": [
    "`LogisticRegression` is an algorithm used for both classification and regression. This algorithm is imported from Scikit-learn. We will use it for emotion classification.\n",
    "\n",
    "Machine learning models have a problem comprehending raw text, they work well with numbers. Machines cannot process the raw text data, and it has to be converted into a matrix of numbers. `CountVectorizer` is used to convert the raw text into a matrix of numbers. This process depends on the frequency of each word in the entire text. During this process, `CountVectorizer` extracts important features from the text. They are used as input for the model during training.\n",
    "\n",
    "The `train_test_split` method is important during the splitting of the dataset. It splits the dataset set into two sets, a `train set`, and a `test set`. This depends on the percentage specified by the user.\n",
    "\n",
    "The `accuracy_score` is important when calculating the accuracy score of our model during prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b90e16d-7c3a-4868-89d9-597ea61e2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8acda4-f1c8-4de4-8f5e-22f7529697b3",
   "metadata": {},
   "source": [
    "## 4. Model features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb560a-ab3c-4d39-b7dc-03d4e8043e26",
   "metadata": {},
   "source": [
    "**Features** are the attributes and variables extracted from the dataset. These extracted features are used as inputs to the model during training enabling model learning. Our features are present in the Clean_Text column.\n",
    "\n",
    "**Labels** are the output or the target variable. Our label is the Emotion column, and this is what the model is predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e1fbbaa-3863-4fdc-a976-a85a60429522",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfeatures = df['Clean_Text']\n",
    "ylabels = df['Emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a55b7e-cba4-4268-a613-8a127f50c40a",
   "metadata": {},
   "source": [
    "## 5. Data splitting and pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d7e70-e0b6-443c-bcec-111461241df5",
   "metadata": {},
   "source": [
    "We need to split our dataset into a train set and test set. The model will learn from the train set. We will use the test set to evaluate the model performance and measure the model’s knowledge capability.\n",
    "\n",
    "We specify the test_size=0.3. This will split our dataset with 70% of data used for training and 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35e25f57-a2a0-4e9e-9561-06ed859a83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "x_train,x_test,y_train,y_test = train_test_split(Xfeatures,ylabels,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080ccda-783d-43f1-8a7c-22c6a138803b",
   "metadata": {},
   "source": [
    "The first stage is the CountVectorizer process. This stage converts the raw text dataset into a matrix of numbers that a machine can understand.\n",
    "\n",
    "The second stage is the model training process using the LogisticRegression algorithm. In this stage, the model learns from the dataset. During training, it understands patterns, gains knowledge, and uses the knowledge to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4feeec2e-b9de-4977-92c9-df879c4af40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e3d58d9-eebd-4af8-8fc2-8046a1303961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogistiticRegression pipeline\n",
    "pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb3af0-7588-40fa-bc10-514ce33b1a25",
   "metadata": {},
   "source": [
    "## 7. Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0de0e-7625-462b-bf3f-57411507754f",
   "metadata": {},
   "source": [
    "After initializing the two stages, we need to fit these stages into our dataset. We will use the train set dataset, which is specified as x_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26a9928f-60f8-41a0-ba53-9fba606538c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and fit data\n",
    "pipe_lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21870e50-d837-43b4-ae22-bd293f9bca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression())])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5a909e5-f16a-40f8-b74c-765dda30b922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6421012122378296"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check accuracy\n",
    "pipe_lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce14ca8a-056d-40e8-b4ca-82dbbc30ef02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness',\n",
       "       'surprise'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr[\"lr\"].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "156f06e0-118c-4781-b868-1f02a33dc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "sample1 = \"This chocolate was very sweet it made me happy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "195a5db1-6d9b-40a3-a889-49ffd4417419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual prediction\n",
    "pipe_lr.predict([sample1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d5aa822-5f22-44b5-99e0-495551c1fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"this is all wrong I shouldn't be up here I should be back in school on the other side of the ocean yet you all come to us young people for Hope how dare you you have stolen my dreams and my childhood with your empty words and yet I'm one of the lucky ones people are suffering people are dying entire ecosystems are collapsing we are in the beginning of a mass extinction and all you can talk about is the money and fairy tales of Eternal economic growth how dare you\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a77e3c25-7bd7-421b-ba3f-6fd4f3d98cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    emotion  percentage\n",
      "0      fear       68.44\n",
      "1   sadness       17.33\n",
      "2     anger       13.81\n",
      "3   disgust        0.32\n",
      "4       joy        0.09\n",
      "5  surprise        0.01\n",
      "6   neutral        0.00\n"
     ]
    }
   ],
   "source": [
    "# show ordered output with emotion name and percentage \n",
    "\n",
    "#show all emotion classes and their probablities together \n",
    "lst_1 = [(clss, prob) for clss, prob in zip(pipe_lr[\"lr\"].classes_, pipe_lr.predict_proba([text_1])[0])]\n",
    "# lst to df\n",
    "df_1 = pd.DataFrame (lst_1)\n",
    "# rename columns of df\n",
    "df_1.rename(columns={0:'emotion', 1: 'percentage'}, inplace = True)\n",
    "# sort values\n",
    "df_1.sort_values(by=\"percentage\", inplace = True, ascending=False)\n",
    "# reset index\n",
    "df_1.reset_index(drop = True, inplace = True)\n",
    "# show percentage\n",
    "df_1['percentage'] = df_1['percentage'].apply(lambda x:round(x, 4)*100)\n",
    "print(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcbef3b6-0ee8-4501-a52b-c0cdda4bec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = \"thank you thank you thank you thank you to the academy for this all 6,000 members thank you to the other nominees all these performances were impeccable in my opinion I didn't see a false note anywhere I want to thank valet or director Jennifer garnered with daily there's a few things about three things to my account that I need each day one of them is something to look up to another is something to look forward to in another is someone to chase now first off I want to thank God because that's who I look up to he's great my life with opportunities that I know are not of my hand or any other human hand he is showing me that it's a scientific fact that gratitude reciprocates\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c09dbec-810e-4749-8704-5d37a14e00aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    emotion  percentage\n",
      "0       joy       99.06\n",
      "1     anger        0.93\n",
      "2   sadness        0.00\n",
      "3      fear        0.00\n",
      "4  surprise        0.00\n",
      "5   disgust        0.00\n",
      "6   neutral        0.00\n"
     ]
    }
   ],
   "source": [
    "# show desc ordered output with emotion name and percentage \n",
    "\n",
    "#show all emotion classes and their probablities together \n",
    "lst_2 = [(clss, prob) for clss, prob in zip(pipe_lr[\"lr\"].classes_, pipe_lr.predict_proba([text_2])[0])]\n",
    "# lst to df\n",
    "df_2 = pd.DataFrame (lst_2)\n",
    "# rename columns of df\n",
    "df_2.rename(columns={0:'emotion', 1: 'percentage'}, inplace = True)\n",
    "# sort values\n",
    "df_2.sort_values(by=\"percentage\", inplace = True, ascending=False)\n",
    "# reset index\n",
    "df_2.reset_index(drop = True, inplace = True)\n",
    "# show percentage\n",
    "df_2['percentage'] = df_2['percentage'].apply(lambda x:round(x, 4)*100)\n",
    "print(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac07bbb0-9f73-4dc2-b1cf-ecdafd2978c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = \"I used to think the whole purpose of life was pursuing happiness everyone said the path to happiness was success so I searched for that ideal job that perfect boyfriend that beautiful apartment but instead of ever feeling fulfilled I felt anxious and the drift and I wasn't alone my friends they struggled with this too\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc0c0e69-97f0-4e41-96cc-0f885d51db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    emotion  percentage\n",
      "0   sadness       45.09\n",
      "1       joy       43.31\n",
      "2      fear       11.52\n",
      "3   disgust        0.05\n",
      "4     anger        0.02\n",
      "5  surprise        0.00\n",
      "6   neutral        0.00\n"
     ]
    }
   ],
   "source": [
    "# show desc ordered output with emotion name and percentage \n",
    "\n",
    "#show all emotion classes and their probablities together \n",
    "lst_3 = [(clss, prob) for clss, prob in zip(pipe_lr[\"lr\"].classes_, pipe_lr.predict_proba([text_3])[0])]\n",
    "# lst to df\n",
    "df_3 = pd.DataFrame (lst_3)\n",
    "# rename columns of df\n",
    "df_3.rename(columns={0:'emotion', 1: 'percentage'}, inplace = True)\n",
    "# sort values\n",
    "df_3.sort_values(by=\"percentage\", inplace = True, ascending=False)\n",
    "# reset index\n",
    "df_3.reset_index(drop = True, inplace = True)\n",
    "# show percentage\n",
    "df_3['percentage'] = df_3['percentage'].apply(lambda x:round(x, 4)*100)\n",
    "print(df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37bbed-ca62-41df-a03e-0291fc47d8f0",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12458b28-862a-47b1-b0b5-129ddeaf4cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle\n",
    "filename = 'emo_text_model_cv.sav'\n",
    "pickle.dump(pipe_lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bce9c3f8-a6c8-4eff-a536-9e5204c68e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n"
     ]
    }
   ],
   "source": [
    "# load the model for predictions\n",
    "import pickle\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict([text_1])[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77c832f7-5331-4663-b353-828ef4d0b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy\n"
     ]
    }
   ],
   "source": [
    "# load the model for predictions\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict([text_2])[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d7bb2e0-e8de-4c65-bd12-3d05b89bedbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness\n"
     ]
    }
   ],
   "source": [
    "# load the model for predictions\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict([text_3])[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ba079-f4d6-403a-82b8-236da1359a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
