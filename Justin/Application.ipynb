{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elyesaseidel/motion-to-emotion/blob/main/Justin/Application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import "
      ],
      "metadata": {
        "id": "rXP2ns34tN44"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QZ1AvmembRUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63b2f25-027d-45e0-cb2c-13c4c32aa9b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# !pip install SpeechRecognition\n",
        "# !pip install streamlit -q\n",
        "# !pip install pyngrok\n",
        "# # !pip install ffmpeg-python\n",
        "# !pip install python-settings\n",
        "# !pip install synthesizer\n",
        "# # !pip install inference\n",
        "# # !pip install helper\n",
        "# !pip install SpeechRecognition moviepy\n",
        "# # !pip3 install imageio==2.4.1\n",
        "# # !pip install imageio-ffmpeg\n",
        "# !pip install --upgrade moviepy\n",
        "# # !pip install aspect-based-sentiment-analysis==1.1.0\n",
        "# !pip install deepface\n",
        "# !pip install nltk\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from deepface import DeepFace \n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "# import aspect_based_sentiment_analysis as absa\n",
        "from pyngrok import ngrok\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import speech_recognition as sr\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import base64\n",
        "# from google.colab.output import eval_js\n",
        "# from IPython.display import HTML, Javascript\n",
        "\n",
        "\n",
        "# https://ngrok.com/ to sign in for free to get auth_key \n",
        "# Tutorial to run streamlit on colab\n",
        "# https://www.youtube.com/watch?v=MUD-pBOnvdo \n",
        "ngrok.set_auth_token('2LC761bNPuCZchtb90lWWFD5sor_G1ecaAUAsAYDSjp6FTD5')\n",
        "\n",
        "# mount your drive with colab notebook\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# # how to read data from google drive\n",
        "# df = pd.read_csv('/content/gdrive/MyDrive/From Motion to Emotion/testing.csv')\n",
        "# # save data to google drive\n",
        "# path = '/content/gdrive/MyDrive/From Motion to Emotion/output.csv'\n",
        "# with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "#   df_alt.to_csv(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit code"
      ],
      "metadata": {
        "id": "pyMGcvERx21o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video resourse \\\n",
        "https://www.youtube.com/watch?v=y9Trdafp83U ('There's more to life than being happy | Emily Esfahani Smith') \\\n",
        "https://www.youtube.com/watch?v=KAJsdgTPJpU&t=1s ('Greta Thunberg's full speech to world leaders at UN Climate Action Summit')\\\n",
        "https://www.youtube.com/watch?v=wD2cVhC-63I ('Matthew McConaughey winning Best Actor | 86th Oscars (2014)')"
      ],
      "metadata": {
        "id": "PFkrApsMiTfe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_XKrKyfcsj5",
        "outputId": "717fbb3a-9f90-4280-9211-3329220d366f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Motion_to_Emotion.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Motion_to_Emotion.py \n",
        "import streamlit as st \n",
        "from pyngrok import ngrok\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "import pandas as pd\n",
        "import speech_recognition as sr\n",
        "import base64\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "from deepface import DeepFace \n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "import moviepy.editor as mp\n",
        "r = sr.Recognizer()\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "\n",
        "image = Image.open('/content/gdrive/MyDrive/From Motion to Emotion/emotion_imgage.jpeg')\n",
        "st.image(image)#, caption='Sunrise by the mountains')\n",
        "\n",
        "st.title('Motion to Emotion')\n",
        "\n",
        "\n",
        "\n",
        "option = st.selectbox(\n",
        "    'Select a video to analyse',\n",
        "    ('Choose video',\n",
        "    'Greta Thunberg\\'s speech at UN', \n",
        "    'Emily Esfahani Smith - There\\'s more to life than being happy', \n",
        "    'Matthew McConaughey winning Best Actor 86th Oscars (2014)'))\n",
        "\n",
        "\n",
        "if option == 'Greta Thunberg\\'s speech at UN':\n",
        "  path = '/content/gdrive/MyDrive/From Motion to Emotion/Greta_UN.mp4'\n",
        "  st.subheader('Greta Thunberg\\'s speech at UN')\n",
        "  st.video(path)\n",
        "\n",
        "if option == 'Emily Esfahani Smith - There\\'s more to life than being happy':\n",
        "  path = '/content/gdrive/MyDrive/From Motion to Emotion/Emily_Happy.mp4'\n",
        "  st.subheader('Emily Esfahani Smith - There\\'s more to life than being happy')\n",
        "  st.video(path)\n",
        "\n",
        "if option == 'Matthew McConaughey winning Best Actor 86th Oscars (2014)':\n",
        "  path = '/content/gdrive/MyDrive/From Motion to Emotion/Matthew_Oscars.mp4'\n",
        "  st.subheader('Matthew McConaughey winning Best Actor 86th Oscars (2014)')\n",
        "  st.video(path)\n",
        "\n",
        "\n",
        "\n",
        "if st.button('Start Analysis'):\n",
        "\n",
        "# Speech Recognition \n",
        "    \n",
        "  clip = mp.VideoFileClip(path) \n",
        "  clip.audio.write_audiofile('audio.wav')\n",
        "\n",
        "  audio = sr.AudioFile(\"audio.wav\")\n",
        "\n",
        "  with audio as source:\n",
        "    audio_file = r.record(source)\n",
        "    text = r.recognize_google(audio_file)\n",
        "\n",
        "\n",
        "# Face emotion dectection \n",
        "# Extrac frames from video \n",
        "  video = cv2.VideoCapture(path)\n",
        "      \n",
        "  total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  step = 30 # if step = 10, then only every 10-th frame will be considered and saved\n",
        "\n",
        "  from collections import Counter\n",
        "  count = 0\n",
        "\n",
        "  frame_list = [] \n",
        "\n",
        "  for i in range(total_frames):\n",
        "      ret, frame = video.read()\n",
        "      if ret:\n",
        "          count += 1\n",
        "          if count % step == 0:\n",
        "              frame_list.append(frame)\n",
        "              \n",
        "  video.release()\n",
        "\n",
        "# Analysis emotion from frames\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  emotions_list = []\n",
        "\n",
        "  for picture in frame_list:\n",
        "    objs = DeepFace.analyze(picture, actions = 'emotion', enforce_detection = False)\n",
        "    dominant_emotion = objs[0]['dominant_emotion']\n",
        "    emotions_list.append(dominant_emotion)\n",
        "\n",
        "# Create datafram from the output\n",
        "\n",
        "  df['Emotion'] = Counter(emotions_list).keys()\n",
        "  df['Counts'] = Counter(emotions_list).values()\n",
        "  df['Percentage'] = (df['Counts']/df['Counts'].sum())*100\n",
        "    \n",
        "  df_sorted = df[['Emotion','Percentage']].sort_values(by = 'Percentage', ascending=False)\n",
        "  df_sorted.reset_index(drop = True, inplace=True)\n",
        "\n",
        "#emoton emoji dictionary\n",
        "\n",
        "  emo_dic = {'neutral': ':neutral_face:',\n",
        "            'fear':':anguished:',\n",
        "            'angry': ':angry:',\n",
        "            'sad':':cry:',\n",
        "            'happy':':grin:',\n",
        "            'suprise':':astanished:',\n",
        "            'disgust':':confounded:'\n",
        "            }\n",
        "  text_emo = {'neg' : 'Negative Sentiment',\n",
        "              'neu' : 'Neutral Sentiment',\n",
        "              'pos' : 'Positive Sentiment' \n",
        "              }\n",
        "# Result displaying \n",
        "\n",
        "  with st.container():\n",
        "    st.header('Speech Recognition')\n",
        "    st.write(text)\n",
        "\n",
        "# Text sentiment analysis \n",
        "    sentiment = sia.polarity_scores(text)\n",
        "    keys_to_keep = ['neg', 'neu', 'pos']\n",
        "    sentiment = {k: sentiment[k] for k in keys_to_keep}\n",
        "    st.header('Sentiment of the Text')\n",
        "    st.subheader(text_emo[max(sentiment, key=sentiment.get)])\n",
        "\n",
        "  with st.container():\n",
        "    st.header(\"Face Emotion\")\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    \n",
        "    with col1:\n",
        "      st.subheader('1st')\n",
        "      st.subheader(df_sorted.Emotion[0])\n",
        "      st.subheader(emo_dic[df_sorted.Emotion[0]])\n",
        "      st.subheader(df_sorted.Percentage[0].round(2))\n",
        "\n",
        "    with col2:\n",
        "      st.subheader('2nd')\n",
        "      st.subheader(df_sorted.Emotion[1])\n",
        "      st.subheader(emo_dic[df_sorted.Emotion[1]])\n",
        "      st.subheader(df_sorted.Percentage[1].round(2))\n",
        "\n",
        "    with col3:\n",
        "      st.subheader('3rd')\n",
        "      st.subheader(df_sorted.Emotion[2])\n",
        "      st.subheader(emo_dic[df_sorted.Emotion[2]])\n",
        "      st.subheader(df_sorted.Percentage[2].round(2)) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Ngrok tunnel and connect"
      ],
      "metadata": {
        "id": "Mla1OBNFyLlV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_nj6JkBRcsuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5f8cc1-d8e0-4c09-cee3-f2ef5d3755d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NgrokTunnel: \"http://dd1c-34-125-222-11.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        }
      ],
      "source": [
        "# get tunnel \n",
        "!nohup streamlit run Motion_to_Emotion.py --server.port 80 &\n",
        "url = ngrok.connect(port = '80')\n",
        "print(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio recording "
      ],
      "metadata": {
        "id": "CbKIEcH-47Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# js = Javascript(\"\"\"\n",
        "#   async function recordAudio(){\n",
        "#     const div = document.createElement('div');\n",
        "#     const audio = document.createElement('audio');\n",
        "#     const strtButton = document.createElement('button');\n",
        "#     const stopButton = document.createElement('button');\n",
        "\n",
        "#     strtButton.textContent = 'Start Recording';\n",
        "#     stopButton.textContent = 'Stop Recording';\n",
        "\n",
        "#     document.body.appendChild(div);\n",
        "#     div.appendChild(strtButton);\n",
        "#     div.appendChild(audio);\n",
        "\n",
        "#     const stream = await navigator.mediaDevices.getUserMedia({audio:true});\n",
        "#     let recorder = new MediaRecorder(stream);\n",
        "\n",
        "#     audio.style.display = 'block';\n",
        "#     audio.srcObject = stream;\n",
        "#     audio.controls = true;\n",
        "#     audio.muted = true;\n",
        "\n",
        "#     await new Promise((resolve) => strtButton.onclick = resolve);\n",
        "#       strtButton.replaceWith(stopButton);\n",
        "#       recorder.start();\n",
        "\n",
        "#     await new Promise((resolve) => stopButton.onclick = resolve);\n",
        "#       recorder.stop();\n",
        "#       let recData = await new Promise ((resolve) => recorder.ondataavailable = resolve);\n",
        "#       let arrBuff = await recData.data.arrayBuffer();\n",
        "#       stream.getAudioTracks()[0].stop();\n",
        "#       div.remove()\n",
        "      \n",
        "#       let binaryString = '';\n",
        "#       let bytes = new Uint8Array(arrBuff);\n",
        "#       bytes.forEach((byte) => { binaryString += String.fromCharCode(byte)});\n",
        "\n",
        "#     const url = URL.createObjectURL(recData.data);\n",
        "#     const player = document.createElement('audio');\n",
        "#     player.controls = true;\n",
        "#     player.src = url;\n",
        "#     document.body.appendChild(player);\n",
        "  \n",
        "#   return btoa(binaryString)};\n",
        "#   \"\"\")"
      ],
      "metadata": {
        "id": "IQAmNyDBNWbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Record audio\n",
        "# display(js)\n",
        "# output = eval_js('recordAudio({})')\n",
        "# with open ('audio.wav','wb') as file:\n",
        "#     binary = base64.b64decode(output)\n",
        "#     file.write(binary)\n",
        "# print('Recording saved to:', file.name)"
      ],
      "metadata": {
        "id": "jbuIRc35RXDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DeepFace Emotion Detection"
      ],
      "metadata": {
        "id": "1Uc_jA6X5QaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the video file from a folder \n",
        "video = cv2.VideoCapture('/content/gdrive/MyDrive/From Motion to Emotion/Greta_UN.mp4')\n",
        "    \n",
        "# Get the total number of frames in the video\n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Set the step\n",
        "step = 10 # if step = 10, then only every 10-th frame will be considered and saved\n",
        "\n",
        "# Initialize the counter\n",
        "from collections import Counter\n",
        "count = 0\n",
        "\n",
        "frame_list = [] \n",
        "# Iterate over each frame\n",
        "for i in range(total_frames):\n",
        "    ret, frame = video.read()\n",
        "    if ret:\n",
        "        count += 1\n",
        "        if count % step == 0:\n",
        "            frame_list.append(frame)\n",
        "            #cv2.imwrite('/content/gdrive/MyDrive/From Motion to Emotion/picture/frame{}.jpg'.format(i), frame)\n",
        "video.release()"
      ],
      "metadata": {
        "id": "JgaUDVuG5VWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import JSON #uncomment in case of an error\n",
        "from deepface import DeepFace # to work with faces\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "df = pd.DataFrame()\n",
        "list_frames_emotions = []\n",
        "\n",
        "# video_frames_path = Path('/content/gdrive/MyDrive/From Motion to Emotion/picture')\n",
        "\n",
        "# os.chdir(video_frames_path)\n",
        "\n",
        "# video_frames = glob.glob(\"./*\")\n",
        "\n",
        "for picture in frame_list:\n",
        "\n",
        "    objs = DeepFace.analyze(picture, actions = 'emotion', enforce_detection = False)\n",
        "    dominant_emotion = objs[0]['dominant_emotion']\n",
        "    list_frames_emotions.append(dominant_emotion)\n",
        "\n",
        "df['Emotion'] = Counter(list_frames_emotions).keys()\n",
        "df['Counts'] = Counter(list_frames_emotions).values()\n",
        "df['Percentage'] = (df['Counts']/df['Counts'].sum())*100\n",
        "print(df)\n",
        "print(df[df.Percentage == df.Percentage.max()])\n"
      ],
      "metadata": {
        "id": "IqsefHH05VTS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "35fe624e-146d-4798-c4e9-b6f8ca7f07a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Action: emotion:   0%|          | 0/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c6544915c1f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpicture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepFace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'emotion'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_detection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdominant_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dominant_emotion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlist_frames_emotions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdominant_emotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/deepface/DeepFace.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(img_path, actions, enforce_detection, detector_backend, align, silent)\u001b[0m\n\u001b[1;32m    275\u001b[0m                                         \u001b[0mimg_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                                         \u001b[0memotion_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                                         \u001b[0msum_of_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotion_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **\n        return self(x, training=False)\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py:885 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/input_spec.py:176 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 48, 48]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# most domiant emotion of the face expression\n",
        "print(df[df.Percentage == df.Percentage.max()])"
      ],
      "metadata": {
        "id": "lYQZNpTMBfgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video-Frame-DeepFace-Output (One Code)"
      ],
      "metadata": {
        "id": "laPMpjoBKGgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install deepface\n",
        "import cv2\n",
        "from deepface import DeepFace \n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import os\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "# Extrac frames from video \n",
        "video = cv2.VideoCapture('/content/gdrive/MyDrive/From Motion to Emotion/Greta_UN.mp4')\n",
        "    \n",
        "total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "step = 10 # if step = 10, then only every 10-th frame will be considered and saved\n",
        "\n",
        "from collections import Counter\n",
        "count = 0\n",
        "\n",
        "frame_list = [] \n",
        "\n",
        "for i in range(total_frames):\n",
        "    ret, frame = video.read()\n",
        "    if ret:\n",
        "        count += 1\n",
        "        if count % step == 0:\n",
        "            frame_list.append(frame)\n",
        "            \n",
        "video.release()\n",
        "\n",
        "# Analysis emotion from frames\n",
        "\n",
        "df = pd.DataFrame()\n",
        "emotions_list = []\n",
        "\n",
        "for picture in frame_list:\n",
        "\n",
        "    objs = DeepFace.analyze(picture, actions = 'emotion', enforce_detection = False)\n",
        "    dominant_emotion = objs[0]['dominant_emotion']\n",
        "    emotions_list.append(dominant_emotion)\n",
        "\n",
        "# Create Output\n",
        "df['Emotion'] = Counter(emotions_list).keys()\n",
        "df['Counts'] = Counter(emotions_list).values()\n",
        "df['Percentage'] = (df['Counts']/df['Counts'].sum())*100\n",
        "print(df)\n",
        "print(df[df.Percentage == df.Percentage.max()])\n"
      ],
      "metadata": {
        "id": "pcuTOEfUKSmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "0VQ73lfIBzok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percent = df.Percentage.max().round(2)\n",
        "print (percent, '%')"
      ],
      "metadata": {
        "id": "PGf_VLR05_w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sorted = df[['Emotion','Percentage']].sort_values(by = 'Percentage', ascending=False)\n",
        "df_sorted.reset_index(drop = True, inplace=True)"
      ],
      "metadata": {
        "id": "-MGRsdiQHcEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emo_dic = {'neutral': ':neutral_face:',\n",
        "           'fear':':anguished:',\n",
        "           'angry': ':angry:',\n",
        "           'sad':':cry:',\n",
        "           'happy':':grin:',\n",
        "           'suprise':':astanished:',\n",
        "           'disgust':':confounded:' \n",
        "}\n"
      ],
      "metadata": {
        "id": "3CycikR9PVD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Text from Video"
      ],
      "metadata": {
        "id": "GTgYuJxBctUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install imageio-ffmpeg\n",
        "import speech_recognition as sr\n",
        "from moviepy.audio.AudioClip import AudioClip\n",
        "from moviepy.audio.io.readers import FFMPEG_AudioReader"
      ],
      "metadata": {
        "id": "GNCLun60dEbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audioclip = AudioFileClip('/content/gdrive/MyDrive/From Motion to Emotion/Greta_UN.mp4')\n",
        "# audioclip.write_audiofile(f\"audio_from_video_sm.wav\")\n",
        "# audio_to_be_text = (f\"audio_from_video_sm.wav\")"
      ],
      "metadata": {
        "id": "lQpX4EV8cy9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = sr.Recognizer()\n",
        "\n",
        "# open the file\n",
        "with sr.AudioFile(audio_to_be_text) as source:\n",
        "    # listen for the data (load audio to memory)\n",
        "    audio_data = r.record(source)\n",
        "    # recognize (convert from speech to text)\n",
        "    text = r.recognize_google(audio_data)\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "DnAttSz7c2y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new approach"
      ],
      "metadata": {
        "id": "3pns2aonjhwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition moviepy\n",
        "!pip3 install imageio==2.4.1\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install --upgrade moviepy\n",
        "import speech_recognition as sr \n",
        "import moviepy.editor as mp\n",
        "r = sr.Recognizer()"
      ],
      "metadata": {
        "id": "kqJaQUkffP6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip = mp.VideoFileClip('/content/gdrive/MyDrive/From Motion to Emotion/Greta_UN.mp4') \n",
        "clip.audio.write_audiofile('Greta_UN.wav')\n",
        "\n",
        "audio = sr.AudioFile(\"Greta_UN.wav\")\n",
        "\n",
        "with audio as source:\n",
        "  audio_file = r.record(source)\n",
        "  result = r.recognize_google(audio_file)\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "csL7AT5DfgZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio = sr.AudioFile(\"conveGreta_UNrted.wav\")"
      ],
      "metadata": {
        "id": "wkEnE2OUhUfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with audio as source:\n",
        "  audio_file = r.record(source)\n",
        "  result = r.recognize_google(audio_file)"
      ],
      "metadata": {
        "id": "Rmw7UCTZhUcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "QK9ioSmRlWN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZR83PaulWJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exporting the result \n",
        "with open('recognized.txt',mode ='w') as file: \n",
        "   file.write(\"Recognized Speech:\") \n",
        "   file.write(\"\\n\") \n",
        "   file.write(result) \n",
        "   print(\"ready!\")"
      ],
      "metadata": {
        "id": "TOq8YXGQhUSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 new approach"
      ],
      "metadata": {
        "id": "_iu7Yfqlj25i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tkinter import *\n",
        "import moviepy.editor as mp \n",
        "import speech_recognition as sr\n",
        "\n",
        "\n",
        "root=Tk()\n",
        "root.geometry('750x420+480+0')\n",
        "root.resizable(False,False)\n",
        "root.config(bg='dark slate gray')\n",
        "root.title('Video To Text Converter')\n",
        "\n",
        "def convert():\n",
        "    clip = mp.VideoFileClip(entry_vid.get()) \n",
        "    clip.audio.write_audiofile(entry_file.get()) \n",
        "        \n",
        "def text():\n",
        "    r = sr.Recognizer()\n",
        "    audio = sr.AudioFile(entry_file.get())\n",
        "    with audio as source:        \n",
        "        audtext = r.listen(source)\n",
        "        try:            \n",
        "            text1 = r.recognize_google(audtext)\n",
        "            st1.set(text1)\n",
        "        except:\n",
        "            st1.set('Try Again!')\n",
        "   \n",
        "def EXIT():\n",
        "    root.destroy()\n",
        "     \n",
        "    \n",
        "\n",
        "\n",
        "# Ltitle = Label(root,text='Video to Text Converter',font='Courier 16 underline bold',bg='dark slate gray',fg='mint cream').grid(row=0,column=2,ipadx=70)\n",
        "\n",
        "# Lvid = Label(root,text=\"Video Path :\",font='Constantia 14',bg='dark slate gray',fg='mint cream').grid(row=2,column=1,ipady=24,ipadx=18)\n",
        "# entry_vid = Entry(root,width=70)\n",
        "# entry_vid.insert(0,\"FILE PATH .mp4\")\n",
        "# entry_vid.place(x=150,y=60)\n",
        "# btn_con = Button(root,text='Convert to Text',font='Constantia 10',fg='maroon',command=convert).place(x=600,y=55)\n",
        "\n",
        "# Lfile = Label(root,text='File Name :',font='Constantia 14',bg='dark slate gray',fg='mint cream').grid(row=3,column=1,ipady=8,ipadx=18)\n",
        "# entry_file = Entry(root,width=70)\n",
        "# entry_file.insert(0,\"FILE NAME .wav\")\n",
        "# entry_file.place(x=150,y=120)\n",
        "\n",
        "# st1 =StringVar()\n",
        "# Ltext = Label(root,text=\"Text :\",font='Constantia 14',bg='dark slate gray',fg='mint cream').grid(row=4,column=1)\n",
        "# entry_text = Entry(root,width=70,textvariable=st1).grid(row=4,column=2,ipady=85)\n",
        "# btn_text = Button(root,text='Get Text',font='Constantia 10',command=text,fg='maroon').place(x=300,y=360)\n",
        "\n",
        "\n",
        "# butexit = Button(root,text= \"Exit\",command=EXIT,font='Constantia 10',fg='maroon').place(x=380,y=360)\n",
        "\n",
        "\n",
        "# root.mainloop()\n",
        "\n",
        "               \n",
        "  "
      ],
      "metadata": {
        "id": "Z0NqkD5LkJeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aspect-Based Sentiment Analysis(ABSA)"
      ],
      "metadata": {
        "id": "pDQlwH68jcpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install aspect-based-sentiment-analysis\n",
        "# !pip uninstall aspect-based-sentiment-analysis==1.1.0\n",
        "# import aspect_based_sentiment_analysis as absa"
      ],
      "metadata": {
        "id": "cPPqOSQsZLLc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nlp = absa.load() \n",
        "# text_1 = (\"this is all wrong I shouldn't be up here I should be back in school on the other side of the ocean yet you all come to us young people for Hope how dare you you have stolen my dreams and my childhood with your empty words and yet I'm one of the lucky ones people are suffering people are dying entire ecosystems are collapsing we are in the beginning of a mass extinction and all you can talk about is the money and fairy tales of Eternal economic growth how dare you\")\n",
        "# people, childhood, ecosystem  = nlp(text_1, aspects = ['people','childhood','ecosystem'])\n",
        "# print(people.sentiment)\n",
        "# print(childhood.sentiment)\n",
        "# print(ecosystem.sentiment)"
      ],
      "metadata": {
        "id": "iY7Uq8A1ipdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"people\", people.sentiment)\n",
        "# print(\"dreams\", dreams.sentiment)\n",
        "# print(\"childhood\", childhood.sentiment)\n",
        "# print(\"ecosystem\", ecosystem.sentiment)\n",
        "# print(\"mass exstinction\", mass_extinction.sentiment)\n",
        "# print(\"fairy tales\", fairy_tales.sentiment)\n",
        "# print(\"ecnonomic growth\", economic_growth.sentiment)"
      ],
      "metadata": {
        "id": "qe2ciqgzlukW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # importance \n",
        "# completed_task = nlp(text_3, aspects=['happiness', 'success', 'job', 'boyfriend', 'apartment', 'feeling', 'friends'])\n",
        "# happiness, success, job, boyfriend, apartment, feeling, friends = completed_task.examples\n",
        "\n",
        "# absa.summary(happiness)\n",
        "# absa.display(happiness.review)\n",
        "\n",
        "# absa.summary(success)\n",
        "# absa.display(success.review)\n",
        "\n",
        "# absa.summary(job)\n",
        "# absa.display(job.review)\n",
        "\n",
        "# absa.summary(boyfriend)\n",
        "# absa.display(boyfriend.review)\n",
        "\n",
        "# absa.summary(apartment)\n",
        "# absa.display(feeling.review)\n",
        "\n",
        "# absa.summary(friends)\n",
        "# absa.display(friends.review)"
      ],
      "metadata": {
        "id": "OSwv8Q0-kNee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simply Sentiment Analysis"
      ],
      "metadata": {
        "id": "-t0qxDUWs_DA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_emo = {'neg' : 'Negative Sentiment',\n",
        "              'neu' : 'Neutral Sentiment',\n",
        "              'pos' : 'Positive Sentiment' \n",
        "              }"
      ],
      "metadata": {
        "id": "_qLFJ3qxxRnu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # !pip install nltk\n",
        "# import nltk\n",
        "# nltk.download('vader_lexicon')\n",
        "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "sentiment = sia.polarity_scores('It is beutiful day')\n",
        "sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W96wpIvEtFal",
        "outputId": "8a397dd7-a2ed-41d7-b897-9e9423cc090b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_to_keep = ['neg', 'neu', 'pos']\n",
        "sentiment = {k: sentiment[k] for k in keys_to_keep}\n",
        "#print(max(sentiment, key=sentiment.get))\n",
        "print(text_emo[max(sentiment, key=sentiment.get)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN2B53AJtXcv",
        "outputId": "3690ea53-06d1-4d91-ca0f-9be6d916ea77"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neutral Sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sO6TMRaN1NKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Emotion Detection "
      ],
      "metadata": {
        "id": "WK9dkstV1N9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "QrQM2cYw1Qdt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neattext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1R1oJk21Qam",
        "outputId": "b2aeb958-055a-4e1c-f98a-5e5fc4886028"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neattext\n",
            "  Downloading neattext-0.1.3-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neattext\n",
            "Successfully installed neattext-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import neattext.functions as nfx\n"
      ],
      "metadata": {
        "id": "y8SZJtRU207e"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is all wrong I shouldn't be up here I should be back in school on the other side of the ocean yet you all come to us young people for Hope how dare you you have stolen my dreams and my childhood with your empty words and yet I'm one of the lucky ones people are suffering people are dying entire ecosystems are collapsing we are in the beginning of a mass extinction and all you can talk about is the money and fairy tales of Eternal economic growth how dare you\""
      ],
      "metadata": {
        "id": "cS3dSyrM2URj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "w9umzgX32kYt",
        "outputId": "819a2ed6-ed29-4c6e-ea65-d4497f3a98d3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"this is all wrong I shouldn't be up here I should be back in school on the other side of the ocean yet you all come to us young people for Hope how dare you you have stolen my dreams and my childhood with your empty words and yet I'm one of the lucky ones people are suffering people are dying entire ecosystems are collapsing we are in the beginning of a mass extinction and all you can talk about is the money and fairy tales of Eternal economic growth how dare you\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimators\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Transformers\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "rB6CEhfZ30yh"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])"
      ],
      "metadata": {
        "id": "mSplUixc30v-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self._vectorizer = vectorizer\n",
        "input_counts = self._vectorizer.transform(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "D-Pe5OuI4dhE",
        "outputId": "6b9e677e-5465-4a60-e87f-6b5de54b2da6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-26bcbe02b22a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_lr.predict([text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "GFKSIxW830tQ",
        "outputId": "e81ea4b2-0151-4cbf-a16f-a4b94718cf35"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-bde8c1e9ebdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             )\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YA7d0qal30q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rX4ZbiiZ30oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LbipIzmY30lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio to Spectrom"
      ],
      "metadata": {
        "id": "Rmcc5wqYyl3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yPxHmCFzMtL",
        "outputId": "51d090ab-ecf0-4088-f053-ec8542daca7f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.11.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (23.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (6.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (2.28.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "# path_save = r'C:\\Users\\Olia\\Desktop\\Lips Reading\\Day5(Video+Audio_camera, CPU)\\CNN_Checkpoints\\test_spec.jpeg'\n",
        "path_load = '/content/gdrive/MyDrive/From Motion to Emotion/03-01-01-01-01-01-01.wav'\n",
        "y, sr = librosa.load(path_load)\n",
        "yt, = librosa.effects.trim(y)\n",
        "# y = yt\n",
        "# mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024, hop_length=100)\n",
        "# mel_spect = librosa.power_to_db(mel_spect, ref=np.max)\n",
        "# aa = librosa.display.specshow(mel_spect, y_axis='mel', fmax=20000, x_axis='time');\n",
        "# # plt.savefig(path_save)\n",
        "# aa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Hl1XOv4nypI5",
        "outputId": "40242e19-5360-4f54-83f4-04d4513cb526"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-7f27dc11ca64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/From Motion to Emotion/03-01-01-01-01-01-01.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# y = yt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=1024, hop_length=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "Ua4mofpNypfx",
        "outputId": "0ba64583-ee6a-4e5a-8d7f-6f48540c908e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-603f75fbf145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'yt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9bGVRNKv1Hbv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iIdZ99oOypdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4SeO04cypaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CbKIEcH-47Xj",
        "GTgYuJxBctUA"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}